{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitanaconda3virtualenve0beec8455a449faa64632dc843014f3",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering a MultiLayer Perceptron using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i found the data set floating around the internet and placed it all in a txt file. One with reviews and the other with the labels\n",
    "g = open('reviews.txt','r') \n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') \n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "25000"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#we can see that the data set is the same exact size. \n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love scenes in clothes warehouse are second to none . the corn on face is a classic  as good as anything in blazing saddles . the take on lawyers is also superb . after being accused of being a turncoat  selling out his boss  and being dishonest the lawyer of pepto bolt shrugs indifferently  i  m a lawyer  he says . three funny words . jeffrey tambor  a favorite from the later larry sanders show  is fantastic here too as a mad millionaire who wants to crush the ghetto . his character is more malevolent than usual . the hospital scene  and the scene where the homeless invade a demolition site  are all  time classics . look for the legs scene and the two big diggers fighting  one bleeds  . this movie gets better each time i see it  which is quite often  .  '"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# lets read\n",
    "reviews[4]\n",
    "# im going to hypothesize that this is a positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'POSITIVE'"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# lets check\n",
    "labels[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even just reading one review will further connect and engage you. With one review you can begin to develop a sense of the data landscape in your set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter objects to store positive, negative and total counts\n",
    "# instantiate empty\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate counters with respective words\n",
    "for i in range(len(reviews)):\n",
    "    if(labels[i] == 'POSITIVE'):\n",
    "        for word in reviews[i].split(' '):\n",
    "            positive_counts[word] += 1\n",
    "            total_counts[word] += 1\n",
    "    else: \n",
    "        for word in reviews[i].split(' '):\n",
    "            negative_counts[word] +=1\n",
    "            total_counts[word] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('', 550468),\n ('the', 173324),\n ('.', 159654),\n ('and', 89722),\n ('a', 83688),\n ('of', 76855),\n ('to', 66746),\n ('is', 57245),\n ('in', 50215),\n ('br', 49235)]"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# check counts of the most common words in positive reviews\n",
    "positive_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('', 561462),\n ('.', 167538),\n ('the', 163389),\n ('a', 79321),\n ('and', 74385),\n ('of', 69009),\n ('to', 68974),\n ('br', 52637),\n ('is', 50083),\n ('it', 48327)]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# check counts of the most common words in negative reviews\n",
    "negative_counts.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right away we can see that there is a lot of noise in this data. If we aren't careful, the high amount of spaces, periods, common words and articles could over set weights, obscure pattern from being detected and cause the model to under perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object to store positive/negative ratios\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for term,cnt in list(total_counts.most_common()):\n",
    "    if(cnt > 100):\n",
    "        pos_neg_ratio = positive_counts[term] / float(negative_counts[term]+1)\n",
    "        pos_neg_ratios[term] = pos_neg_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pos-to-neg ratio for 'and' = 1.2061678272793268\nPos-to-neg ratio for 'fantastic' = 4.503448275862069\nPos-to-neg ratio for 'disgusting' = 0.32142857142857145\nPos-to-neg ratio for 'terrible' = 0.17744252873563218\n"
    }
   ],
   "source": [
    "print(\"Pos-to-neg ratio for 'and' = {}\".format(pos_neg_ratios[\"and\"]))\n",
    "print(\"Pos-to-neg ratio for 'fantastic' = {}\".format(pos_neg_ratios[\"fantastic\"]))\n",
    "print(\"Pos-to-neg ratio for 'disgusting' = {}\".format(pos_neg_ratios[\"disgusting\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ratios to logs\n",
    "for word,ratio in pos_neg_ratios.most_common():\n",
    "    pos_neg_ratios[word] = np.log(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Pos-to-neg ratio for 'and' = 0.18744824888788403\nPos-to-neg ratio for 'fantastic' = 1.5048433868558566\nPos-to-neg ratio for 'disgusting' = -1.1349799328389845\nPos-to-neg ratio for 'terrible' = -1.7291085042663878\n"
    }
   ],
   "source": [
    "# now rather than high and low numbers, the two categories are polarized\n",
    "# 0 is neutral | positive is +integers | negative is -integers\n",
    "print(\"Pos-to-neg ratio for 'and' = {}\".format(pos_neg_ratios[\"and\"]))\n",
    "print(\"Pos-to-neg ratio for 'fantastic' = {}\".format(pos_neg_ratios[\"fantastic\"]))\n",
    "print(\"Pos-to-neg ratio for 'disgusting' = {}\".format(pos_neg_ratios[\"disgusting\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('edie', 4.6913478822291435),\n ('paulie', 4.07753744390572),\n ('felix', 3.152736022363656),\n ('polanski', 2.8233610476132043),\n ('matthau', 2.80672172860924),\n ('victoria', 2.681021528714291),\n ('mildred', 2.6026896854443837),\n ('gandhi', 2.538973871058276),\n ('flawless', 2.451005098112319),\n ('superbly', 2.26002547857525),\n ('perfection', 2.159484249353372),\n ('astaire', 2.1400661634962708),\n ('captures', 2.038619547159581),\n ('voight', 2.030170492673053),\n ('wonderfully', 2.0218960560332353),\n ('powell', 1.978345424808467),\n ('brosnan', 1.9547990964725592),\n ('lily', 1.9203768470501485),\n ('bakshi', 1.9029851043382795),\n ('lincoln', 1.9014583864844796),\n ('refreshing', 1.8551812956655511),\n ('breathtaking', 1.8481124057791867),\n ('bourne', 1.8478489358790986),\n ('lemmon', 1.8458266904983307),\n ('delightful', 1.8002701588959635),\n ('flynn', 1.7996646487351682),\n ('andrews', 1.7764919970972666),\n ('homer', 1.7692866133759964),\n ('beautifully', 1.7626953362841438),\n ('soccer', 1.7578579175523736)]"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# words most frequently seen in a review with a \"POSITIVE\" label\n",
    "pos_neg_ratios.most_common()[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some of these words are names. take 'polanski' for example.  mentioning Roman Polanski's name is not an inherent result of a positive sentiment. it is more so a result of the context in which he is being talked about. i can't assume everytime someone has mentioned 'polanski' in text has been in order to express positivity. however - there are some clear gems cutting through the noise. like: 'beautifully, breathtaking, delightful, perfection' -- when building the model, it will be important to address this  and reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('boll', -4.969813299576001),\n ('uwe', -4.624972813284271),\n ('seagal', -3.644143560272545),\n ('unwatchable', -3.258096538021482),\n ('stinker', -3.2088254890146994),\n ('mst', -2.9502698994772336),\n ('incoherent', -2.9368917735310576),\n ('unfunny', -2.6922395950755678),\n ('waste', -2.6193845640165536),\n ('blah', -2.5704288232261625),\n ('horrid', -2.4849066497880004),\n ('pointless', -2.4553061800117097),\n ('atrocious', -2.4259083090260445),\n ('redeeming', -2.3682390632154826),\n ('prom', -2.3608540011180215),\n ('drivel', -2.3470368555648795),\n ('lousy', -2.307572634505085),\n ('worst', -2.286987896180378),\n ('laughable', -2.264363880173848),\n ('awful', -2.227194247027435),\n ('poorly', -2.2207550747464135),\n ('wasting', -2.204604684633842),\n ('remotely', -2.1972245773362196),\n ('existent', -2.0794415416798357),\n ('boredom', -1.995100393246085),\n ('miserably', -1.9924301646902063),\n ('sucks', -1.987068221548821),\n ('uninspired', -1.9832976811269336),\n ('lame', -1.981767458946166)]"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# words most frequently seen in a review with a \"NEGATIVE\" label\n",
    "pos_neg_ratios.most_common()[:-30:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these words seem more related directly to negative sentiment. except for one name that is peaking as the third highest word related to negative sentiment. *shrug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "74074\n"
    }
   ],
   "source": [
    "# object containing all words from all of the reviews\n",
    "vocab = set(total_counts.keys())\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 74074)"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# this is how the first input will to pass through the model\n",
    "layer_0 = np.zeros((1,vocab_size))\n",
    "layer_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local library housing the custom model with two hidden layers\n",
    "import mfinchmods as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "How much I've read:0.0% How fast I can read:(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%\nHow much I've read:10.4% How fast I can read:(reviews/sec):1159. #Correct:1994 #Trained:2501 Training Accuracy:79.7%\nHow much I've read:20.8% How fast I can read:(reviews/sec):1135. #Correct:4063 #Trained:5001 Training Accuracy:81.2%\nHow much I've read:31.2% How fast I can read:(reviews/sec):1143. #Correct:6176 #Trained:7501 Training Accuracy:82.3%\nHow much I've read:41.6% How fast I can read:(reviews/sec):1147. #Correct:8336 #Trained:10001 Training Accuracy:83.3%\nHow much I've read:52.0% How fast I can read:(reviews/sec):1141. #Correct:10501 #Trained:12501 Training Accuracy:84.0%\nHow much I've read:62.5% How fast I can read:(reviews/sec):1146. #Correct:12641 #Trained:15001 Training Accuracy:84.2%\nHow much I've read:72.9% How fast I can read:(reviews/sec):1147. #Correct:14782 #Trained:17501 Training Accuracy:84.4%\nHow much I've read:83.3% How fast I can read:(reviews/sec):1142. #Correct:16954 #Trained:20001 Training Accuracy:84.7%\nHow much I've read:93.7% How fast I can read:(reviews/sec):1142. #Correct:19143 #Trained:22501 Training Accuracy:85.0%\nHow much I've read:99.9% How fast I can read:(reviews/sec):1143. #Correct:20461 #Trained:24000 Training Accuracy:85.2%"
    }
   ],
   "source": [
    "# meet Senti_Net\n",
    "nn_clf = mf.Senti_Net(reviews[:-1000], #splitting the data manually, leaving the last 1000 for testing\n",
    "                      labels[:-1000],\n",
    "                      min_count=20, # here is how i addressed the noise. words or characters that appeared either too much or not enough to actually by part of a pattern\n",
    "                      polarity_cutoff=0.05,\n",
    "                      learning_rate=0.01)\n",
    "nn_clf.train(reviews[:-1000],labels[:-1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traing went pretty well. I'll review the model's architecture in another part of the article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "How much I've read:99.9% How fast I can read:(reviews/sec):1568. #Correct:859 #Tested:1000 Testing Accuracy:85.9%"
    }
   ],
   "source": [
    "nn_clf.predict(reviews[-1000:], labels[-1000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Senti_Net did better than the Kera's models; by roughly 2% than the over complex model and roughly 3% than the simpler model. Not a lot, but it says more about knowing your data and building a model to directly address the patterns in the data it is working with. To do this, you need to be engaged with your data as well as have intentional control over the mechanisms of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'NEGATIVE'"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "nn_clf.classify(reviews[1289])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "i saw this last week after picking up the dvd cheap . i had wanted to see it for ages  finding the plot outline very intriguing . so my disappointment was great  to say the least . i thought the lead actor was very flat . this kind of part required a performance like johny depp  s in the ninth gate  of which this is almost a complete rip  off   but i guess tv budgets don  t always stretch to this kind of acting ability .  br    br   i also the thought the direction was confused and dull  serving only to remind me that carpenter hasn  t done a decent movie since in the mouth of madness . as for the story  well  i was disappointed there as well  there was no way it could meet my expectation i guess  but i thought the payoff and explanation was poor  and the way he finally got the film anti  climactic to say the least .  br    br   this was written by one of the main contributors to aicn  and you can tell he does love his cinema  but i would have liked a better result from such a good initial premise .  br    br   i took the dvd back to the store the same day   \n"
    }
   ],
   "source": [
    "print(reviews[1289])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building out Further Functionality Using Senti_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a newtwork that doesn't us polarity or min_cut offf\n",
    "nn_clf_f = mf.Senti_Net(reviews[:-1000],\n",
    "                        labels[:-1000],min_count=0,\n",
    "                        polarity_cutoff=0,\n",
    "                        learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "How much I've read:0.0% How fast I can read:(reviews/sec):0.0 #Correct:1 #Trained:1 Training Accuracy:100.%\nHow much I've read:10.4% How fast I can read:(reviews/sec):914.5 #Correct:1962 #Trained:2501 Training Accuracy:78.4%\nHow much I've read:20.8% How fast I can read:(reviews/sec):897.0 #Correct:4002 #Trained:5001 Training Accuracy:80.0%\nHow much I've read:31.2% How fast I can read:(reviews/sec):907.9 #Correct:6120 #Trained:7501 Training Accuracy:81.5%\nHow much I've read:41.6% How fast I can read:(reviews/sec):913.4 #Correct:8271 #Trained:10001 Training Accuracy:82.7%\nHow much I've read:52.0% How fast I can read:(reviews/sec):912.6 #Correct:10431 #Trained:12501 Training Accuracy:83.4%\nHow much I've read:62.5% How fast I can read:(reviews/sec):909.2 #Correct:12565 #Trained:15001 Training Accuracy:83.7%\nHow much I've read:72.9% How fast I can read:(reviews/sec):908.2 #Correct:14670 #Trained:17501 Training Accuracy:83.8%\nHow much I've read:83.3% How fast I can read:(reviews/sec):908.1 #Correct:16833 #Trained:20001 Training Accuracy:84.1%\nHow much I've read:93.7% How fast I can read:(reviews/sec):904.9 #Correct:19015 #Trained:22501 Training Accuracy:84.5%\nHow much I've read:99.9% How fast I can read:(reviews/sec):906.0 #Correct:20335 #Trained:24000 Training Accuracy:84.7%"
    }
   ],
   "source": [
    "nn_clf_f.train(reviews[:-1000],\n",
    "              labels[:-1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use weights from the trained the model to express how the model has clustered like words in it's learning\n",
    "def fetch_similar_words_to(focus = str):\n",
    "    most_similar = Counter()\n",
    "\n",
    "    for word in nn_clf_f.word2dict.keys():\n",
    "        most_similar[word] = np.dot(nn_clf_f.weights_0_1[nn_clf_f.word2dict[word]],\n",
    "                                    nn_clf_f.weights_0_1[nn_clf_f.word2dict[focus]])\n",
    "    \n",
    "    return most_similar.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('excellent', 0.08714175888229203),\n ('perfect', 0.07997393832571632),\n ('amazing', 0.058524466856635544),\n ('today', 0.05750220855411224),\n ('wonderful', 0.05694920677561696),\n ('fun', 0.05576918451159403),\n ('great', 0.055538020108908126),\n ('best', 0.05468981521759843),\n ('liked', 0.049518996908511824),\n ('definitely', 0.048837788647917935)]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# example of using similarity function\n",
    "fetch_similar_words_to('great')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that embedded in the weights, the connections between neurons, is distilled realtions that can be used in customizable ways to further explore either dataset or the sentimentality of clustered data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Data\n",
    "#### Bringing in Completey foreign Dataset to see how the Model perceives it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in a twitter data that I've been mining for a seperate project\n",
    "import pandas as pd\n",
    "tw = pd.read_csv('twitter_data.csv')\n",
    "tweets = list(tw['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'NEGATIVE'"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "# classify random tweet from set\n",
    "nn_clf.classify(tweets[321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "You can sue tobacco companies. \n\nYou can sue pharmaceutical companies.\n\nThe only companies you can’t sue are gun manufacturers because of a law Bernie Sanders voted for. #DemDebate\n"
    }
   ],
   "source": [
    "# check to see what the tweet is and review how the model performed\n",
    "print(tweets[321])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'POSITIVE'"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "# repeat for validation\n",
    "nn_clf.classify(tweets[187])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Young people have led every movement for justice in our nation's history and they are leading the movement for climate justice now.\n\nPoliticians must listen. https://t.co/FF46Bhguls\n"
    }
   ],
   "source": [
    "print(tweets[187])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would say the model is accurate"
   ]
  }
 ]
}